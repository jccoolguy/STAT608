---
title: "STAT 608 HW 3"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 09/23/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

1\.

a\)

The business analyst claims that this model is highly effective for understanding the effects of Distance on Fare and predicting future values of Fare, there a few issues with this conclusion. First when we look at the standard residual plot we see that there appears to be a discernible quadratic pattern differing from the assumption of i.i.d $N(0,\sigma^2)$ random variables. This violates our ability to create accurate prediction intervals and our ability to provide inference on coefficient estimates since $n=17$.

With our current model there are also two outlier points I have labeled in red below. These need to be further analyzed. If they are valid they could be evidence of non-constant variance

```{r echo=FALSE}
airfares <- read.delim("airfares.txt",sep = "\t", header = TRUE)
attach(airfares)
airfare_slr <- lm(Fare ~ Distance)
```

```{r echo=FALSE, fig.width = 5,fig.height = 3.5}
leverage1 <- hatvalues(airfare_slr)
StanRes1 <- rstandard(airfare_slr)
residual1 <- airfares$residuals
plot(Distance,StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)
points(x = c(Distance[13],Distance[17]),y = c(StanRes1[13],StanRes1[17])
       ,col = "red")
text(x = c(Distance[13],Distance[17]), y = c(StanRes1[13],StanRes1[17]), labels = c("13","17"), cex = .7, pos = c(2,3))
```

b\)

The ordinary straight line regression model seems to fit the data reasonably. To further improve this model first the analyst should assess the validity of the two outlier points 13 and 17. If the data is valid I would take this as evidence of non constant variance and consider introducing a regression model using the square root of the response variable and the square root of distance. I would also consider adding a quadratic term to deal with the pattern in the rest of the residual plot.

#To be removed#

```{r}
airfares_no_outliers <- airfares[c(1:12,14:16),]
```

```{r}
airfares_no <- lm(Fare~Distance, data = airfares_no_outliers)
```

```{r}
plot(airfares_no_outliers$Distance, airfares_no_outliers$Fare)
abline(lsfit(airfares_no_outliers$Distance, airfares_no_outliers$Fare))
```

```{r}
leverage1 <- hatvalues(airfares_no)
StanRes1 <- rstandard(airfares_no)
plot(airfares_no_outliers$Distance,StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)

```

```{r}
airfares_no_outliers$Distance2 <- airfares_no_outliers$Distance^2
```

```{r}
airfares_no_quad <- lm(Fare~Distance + Distance2, data = airfares_no_outliers)
```

```{r}
plot(airfares_no_outliers$Distance, airfares_no_outliers$Fare)
curve(predict(airfares_no_quad, newdata = data.frame(Distance = x, Distance2=x^2)),add = T)
```

```{r}
leverage1 <- hatvalues(airfares_no_quad)
StanRes1 <- rstandard(airfares_no_quad)
plot(airfares_no_outliers$Distance,StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)
```

```{r}
airfares$Distance2 <- airfares$Distance^2
```

```{r}
test <- lm(Fare ~ Distance + Distance2, data = airfares[c(1:12,14:17),])
summary(test)
```

```{r}
plot(x = airfares[c(1:12,14:17),"Distance"], y = airfares[c(1:12,14:17),"Fare"])
curve(predict(test, newdata = data.frame(Distance = x, Distance2 = x^2)),add= T)
```

```{r}
leverage1 <- hatvalues(test)
StanRes1 <- rstandard(test)
plot(airfares[c(1:12,14:17),"Distance"],StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)
```

#To be removed End#

3\)

In question 3 we have the design matrix X of:

$$
X=\begin{bmatrix} 1_m & 0_m \\ 0_{n-m} & 1_{n-m}\end{bmatrix}
$$

We use $H=X(X^TX)^{-1}X^T$:

$$
(X^TX)^{-1}=
$$

4\.

a\)

We know $\hat{e}=y-\hat{y}$ and that $\hat{y}=Hy$. Where H is the projection matrix onto y. So:

$$
\hat{e}=y-\hat{y}=y-Hy=(I-H)y
$$

b\)

We know the below fact where $A$ is a constant matrix and $b$ is a vector of random variables.

$$
Var(Ab)=AVar(b)A^T
$$ So then, using that $(I-H)$ is a matrix and y is a vector of random variables with $Var(y)=\Sigma$.

$$
Var[(I-H)y]=(I-H)Var(y)(I-H)^T=(I-H)\Sigma(I-H)^T
$$

We use the fact that $\Sigma=\sigma^2I$ to continue to simplify:

$$
(I-H)\Sigma(I-H)^T=(I-H)\sigma^2I(I-H)^T=\sigma^2(I-H)(I-H)^T \\
=\sigma^2(II^T-IH^T-HI^T+HH^T)
$$

Using the fact that H,I are symmetric we get.

$$
\sigma^2(I-2H+HH)
$$

We can prove that $HH=H$, we use that $H=X(X^TX)^{-1}X^T$. Then:

$$
HH=X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T=X(X^TX)^{-1}X^T=H
$$

So we get the final result for the covariance matrix of errors:

$$
\sigma^2(I-H)
$$

c\)

We know that the entries in $H$ are $h_{ij}$ in each $(i,j)$ position. And $I$ is the identity matrix and thus diagonal, so its entries are$I_{ij}=0,i \neq j$ and $I_{ij}=1,i=j$ . Then we can say the below:

$$
Cov(\hat{e_i},\hat{e_j})=\sigma^2(I_{ij}-h_{ij})=-h_{ij}\sigma^2,i \neq j
$$

5\.

a\)

We know that $H=X(X^TX)^{-1}X^T$. Then using the rule $(AB)^T=B^TA$ and $((AB)^{-1})^T=((AB)^{T})^{-1}$we have:

$$
H^T=(X(X^TX)^{-1}X^T)^T=X((X^TX)^{T})^{-1}X^T=X(X^TX)^{-1}X^T=H
$$

b\)

H is idempotent, $HH=H$. So:

$$
HH=\begin{bmatrix} h_{11} & h_{12} & \dots & h_{1n} \\ h_{21} &h_{22} & \dots  & h_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ h_{n1} &h_{n2} & \dots & h_{nn} \end{bmatrix}\begin{bmatrix} h_{11} & h_{12} & \dots & h_{1n} \\ h_{21} &h_{22} & \dots  & h_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ h_{n1} &h_{n2} & \dots & h_{nn} \end{bmatrix}=\begin{bmatrix} \sum_{i=j}^nh_{1j}h_{j1} &\sum_{j=1}^nh_{1j}h_{j2} &\dots & \sum_{j=1}^nh_{1j}h_{jn} \\ \sum_{j=1}^nh_{2j}h_{j1} &\sum_{j=1}^nh_{2j}h_{j2} &\dots & \sum_{j=1}^nh_{2j}h_{jn} \\ \vdots & \vdots & \vdots & \vdots \\ \sum_{j=1}^nh_{nj}h_{j1} &\sum_{j=1}^nh_{nj}h_{j2} &\dots & \sum_{j=1}^nh_{nj}h_{jn}\end{bmatrix}=H
$$

$H=H^T$ so $h_{ij}=h_{ji}$. So the diagonal elements of $H$ are $h_{ii}=\sum_{j=1}^n h_{ij}^2$. We can also write this as $h_{ii}=h_{ii}^2+\sum_{j=1,j\neq i}^nh_{ij}^2$. So we can conclude that $h_{ii} \geq h_{ii}^2\geq0$. Additionally the only time that a square of a number is less than or equal to the number itself is when it is less than 1. So therefore $0 \leq h_{ii} \leq 1$.

c\)

We have $H=X(X^TX)^{-1}X^T$. Where X is the design matrix for simple linear regression.

So:

$$
(X^TX)^{-1}=\frac{1}{SXX}\begin{bmatrix}\sum_{i=1}^n x_i^2/n & -\bar{x} \\ -\bar{x} & 1\end{bmatrix}
$$

$$
X(X^TX)^{-1}=\frac{1}{SXX}\begin{bmatrix} \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_1 & -\bar{x}+x_1 \\ \vdots & \vdots \\ \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_n & -\bar{x}+x_n \end{bmatrix}
$$

$$
H=X(X^TX)^{-1}X^T=\frac{1}{SXX}\begin{bmatrix} \sum_{i=1}^n \frac{x_i^2}{n}-\bar{x}x_1+(-\bar{x}+x_1)x_1 & \dots &\sum_{i=1}^n \frac{x_i^2}{n}-\bar{x}x_1+(-\bar{x}+x_1)x_n \\ \vdots & \vdots & \vdots \\ \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_n+(-\bar{x}+x_n)x_1 & \vdots & \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_n+(-\bar{x}+x_n)x_n\end{bmatrix}
$$

Then we can generalize an entry of H, $h_{ij}$ as:

$$
h_{ij}=\frac{1}{SXX}(\sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_i+(-\bar{x}+x_i)x_j)
$$

We can create the desired $1/n$ term by adding and subtracting $n\bar{x}^2/n=\bar{x}^2$. We know that $\sum_{i=1}^nx_i^2+n\bar{x}^2=SXX$:

$$
h_{ij}=\frac{1}{SXX}(\frac{1}{n}(\sum x_i^2-n\bar{x}^2)+\bar{x}^2-\bar{x}x_i-\bar{x}x_j+x_ix_j)
$$

$$
h_{ij}=\frac{1}{n}+\frac{\bar{x}^2-\bar{x}x_i-\bar{x}x_j+x_ix_j}{SXX}
$$

We can factor the numerator into $(x_i-\bar{x})(x_j-\bar{x})$. We get our desired result:

$$
h_{ij}=\frac{1}{n}+\frac{(x_i-\bar{x})(x_j - \bar{x})}{SXX}
$$

d\)

#Work on#
