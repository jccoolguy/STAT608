---
title: "STAT 608 HW 3"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 09/23/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

1\.

a\)

The business analyst claims that this model is highly effective for understanding the effects of Distance on Fare and predicting future values of Fare, there a few issues with this conclusion. First when we look at the standard residual plot we see that there appears to be a discernible quadratic pattern differing from the assumption of i.i.d $N(0,\sigma^2)$ random variables. This violates our ability to create accurate prediction intervals and our ability to provide inference on coefficient estimates since $n=17$.

With our current model there are also two outlier points I have labeled in red below. These need to be further analyzed. If they are valid they could be evidence of non-constant variance

```{r echo=FALSE}
airfares <- read.delim("airfares.txt",sep = "\t", header = TRUE)
attach(airfares)
airfare_slr <- lm(Fare ~ Distance)
```

```{r echo=FALSE, fig.width = 5,fig.height = 3.5}
leverage1 <- hatvalues(airfare_slr)
StanRes1 <- rstandard(airfare_slr)
residual1 <- airfares$residuals
plot(Distance,StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)
points(x = c(Distance[13],Distance[17]),y = c(StanRes1[13],StanRes1[17])
       ,col = "red")
text(x = c(Distance[13],Distance[17]), y = c(StanRes1[13],StanRes1[17]), labels = c("13","17"), cex = .7, pos = c(2,3))
```

b\)

The ordinary straight line regression model seems to fit the data reasonably. To further improve this model first the analyst should assess the validity of the two outlier points 13 and 17. If the data is valid I would take this as evidence of non constant variance and consider introducing a regression model using the square root of the response variable and the square root of distance. I would also consider adding a quadratic term to deal with the pattern in the rest of the residual plot.

#To be removed#

```{r}
airfares_no_outliers <- airfares[c(1:12,14:16),]
```

```{r}
airfares_no <- lm(Fare~Distance, data = airfares_no_outliers)
```

```{r}
plot(airfares_no_outliers$Distance, airfares_no_outliers$Fare)
abline(lsfit(airfares_no_outliers$Distance, airfares_no_outliers$Fare))
```

```{r}
leverage1 <- hatvalues(airfares_no)
StanRes1 <- rstandard(airfares_no)
plot(airfares_no_outliers$Distance,StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)

```

```{r}
airfares_no_outliers$Distance2 <- airfares_no_outliers$Distance^2
```

```{r}
airfares_no_quad <- lm(Fare~Distance + Distance2, data = airfares_no_outliers)
```

```{r}
plot(airfares_no_outliers$Distance, airfares_no_outliers$Fare)
curve(predict(airfares_no_quad, newdata = data.frame(Distance = x, Distance2=x^2)),add = T)
```

```{r}
leverage1 <- hatvalues(airfares_no_quad)
StanRes1 <- rstandard(airfares_no_quad)
plot(airfares_no_outliers$Distance,StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)
```

```{r}
airfares$Distance2 <- airfares$Distance^2
```

```{r}
test <- lm(Fare ~ Distance + Distance2, data = airfares[c(1:12,14:17),])
summary(test)
```

```{r}
plot(x = airfares[c(1:12,14:17),"Distance"], y = airfares[c(1:12,14:17),"Fare"])
curve(predict(test, newdata = data.frame(Distance = x, Distance2 = x^2)),add= T)
```

```{r}
leverage1 <- hatvalues(test)
StanRes1 <- rstandard(test)
plot(airfares[c(1:12,14:17),"Distance"],StanRes1, ylab="Standardized Residuals")
abline(h=2,lty=2)
abline(h=-2,lty=2)
```

#To be removed End#

3\)

In question 3 we have the design matrix X of:

$$
X=\begin{bmatrix} 1_m & 0_m \\ 0_{n-m} & 1_{n-m}\end{bmatrix}
$$

We use $H=X(X^TX)^{-1}X^T$:

$$
(X^TX)^{-1}=\begin{bmatrix} \frac{1}{m} & 0 \\ 0 & \frac{1}{n-m}\end{bmatrix}
$$

$$
X(X^TX)=\begin{bmatrix} (\frac{1}{m})_m &0_m \\ 0 & (\frac{1}{n-m})_{n-m}\end{bmatrix}
$$

$$
H =X(X^TX)^{-1}X^T=\begin{bmatrix} \frac{1}{m} & \frac{1}{m} &\frac{1}{m} & \dots &\frac{1}{m}&0&0&0 & \dots & 0 \\\vdots & \vdots & \vdots & \vdots & \vdots & \vdots& \vdots & \vdots& \vdots & \vdots \\\frac{1}{m} & \frac{1}{m} &\frac{1}{m} & \dots &\frac{1}{m}&0&0&0 & \dots & 0 \\ 0 & 0 & 0 & \dots & 0 & \frac{1}{n-m} & \frac{1}{n-m} & \frac{1}{n-m} & \frac{1}{n-m}& \frac{1}{n-m} \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots& \vdots & \vdots& \vdots & \vdots \\ 0 & 0 & 0 & \dots & 0 & \frac{1}{n-m} & \frac{1}{n-m} & \frac{1}{n-m} & \frac{1}{n-m}& \frac{1}{n-m} \end{bmatrix}
$$

The projection matrix projects the response variable $y$ onto the column space of $X$. In doing this we are finding the closest point $\hat{y}_i$ for each $y_i$. In this particular case we are projecting

4\.

a\)

We know $\hat{e}=y-\hat{y}$ and that $\hat{y}=Hy$. Where H is the projection matrix onto y. So:

$$
\hat{e}=y-\hat{y}=y-Hy=(I-H)y
$$

b\)

We know the below fact where $A$ is a constant matrix and $b$ is a vector of random variables.

$$
Var(Ab)=AVar(b)A^T
$$ So then, using that $(I-H)$ is a matrix and y is a vector of random variables with $Var(y)=\Sigma$.

$$
Var[(I-H)y]=(I-H)Var(y)(I-H)^T=(I-H)\Sigma(I-H)^T
$$

We use the fact that $\Sigma=\sigma^2I$ to continue to simplify:

$$
(I-H)\Sigma(I-H)^T=(I-H)\sigma^2I(I-H)^T=\sigma^2(I-H)(I-H)^T \\
=\sigma^2(II^T-IH^T-HI^T+HH^T)
$$

Using the fact that H,I are symmetric we get.

$$
\sigma^2(I-2H+HH)
$$

We can prove that $HH=H$, we use that $H=X(X^TX)^{-1}X^T$. Then:

$$
HH=X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T=X(X^TX)^{-1}X^T=H
$$

So we get the final result for the covariance matrix of errors:

$$
\sigma^2(I-H)
$$

c\)

We know that the entries in $H$ are $h_{ij}$ in each $(i,j)$ position. And $I$ is the identity matrix and thus diagonal, so its entries are$I_{ij}=0,i \neq j$ and $I_{ij}=1,i=j$ . Then we can say the below:

$$
Cov(\hat{e_i},\hat{e_j})=\sigma^2(I_{ij}-h_{ij})=-h_{ij}\sigma^2,i \neq j
$$

5\.

a\)

We know that $H=X(X^TX)^{-1}X^T$. Then using the rule $(AB)^T=B^TA$ and $((AB)^{-1})^T=((AB)^{T})^{-1}$we have:

$$
H^T=(X(X^TX)^{-1}X^T)^T=X((X^TX)^{T})^{-1}X^T=X(X^TX)^{-1}X^T=H
$$

b\)

H is idempotent, $HH=H$. So:

$$
HH=\begin{bmatrix} h_{11} & h_{12} & \dots & h_{1n} \\ h_{21} &h_{22} & \dots  & h_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ h_{n1} &h_{n2} & \dots & h_{nn} \end{bmatrix}\begin{bmatrix} h_{11} & h_{12} & \dots & h_{1n} \\ h_{21} &h_{22} & \dots  & h_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ h_{n1} &h_{n2} & \dots & h_{nn} \end{bmatrix}=\begin{bmatrix} \sum_{i=j}^nh_{1j}h_{j1} &\sum_{j=1}^nh_{1j}h_{j2} &\dots & \sum_{j=1}^nh_{1j}h_{jn} \\ \sum_{j=1}^nh_{2j}h_{j1} &\sum_{j=1}^nh_{2j}h_{j2} &\dots & \sum_{j=1}^nh_{2j}h_{jn} \\ \vdots & \vdots & \vdots & \vdots \\ \sum_{j=1}^nh_{nj}h_{j1} &\sum_{j=1}^nh_{nj}h_{j2} &\dots & \sum_{j=1}^nh_{nj}h_{jn}\end{bmatrix}=H
$$

$H=H^T$ so $h_{ij}=h_{ji}$. So the diagonal elements of $H$ are $h_{ii}=\sum_{j=1}^n h_{ij}^2$. We can also write this as $h_{ii}=h_{ii}^2+\sum_{j=1,j\neq i}^nh_{ij}^2$. So we can conclude that $h_{ii} \geq h_{ii}^2\geq0$. Additionally the only time that a square of a number is less than or equal to the number itself is when it is less than 1. So therefore $0 \leq h_{ii} \leq 1$.

c\)

We have $H=X(X^TX)^{-1}X^T$. Where X is the design matrix for simple linear regression.

So:

$$
(X^TX)^{-1}=\frac{1}{SXX}\begin{bmatrix}\sum_{i=1}^n x_i^2/n & -\bar{x} \\ -\bar{x} & 1\end{bmatrix}
$$

$$
X(X^TX)^{-1}=\frac{1}{SXX}\begin{bmatrix} \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_1 & -\bar{x}+x_1 \\ \vdots & \vdots \\ \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_n & -\bar{x}+x_n \end{bmatrix}
$$

$$
H=X(X^TX)^{-1}X^T=\frac{1}{SXX}\begin{bmatrix} \sum_{i=1}^n \frac{x_i^2}{n}-\bar{x}x_1+(-\bar{x}+x_1)x_1 & \dots &\sum_{i=1}^n \frac{x_i^2}{n}-\bar{x}x_1+(-\bar{x}+x_1)x_n \\ \vdots & \vdots & \vdots \\ \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_n+(-\bar{x}+x_n)x_1 & \vdots & \sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_n+(-\bar{x}+x_n)x_n\end{bmatrix}
$$

Then we can generalize an entry of H, $h_{ij}$ as:

$$
h_{ij}=\frac{1}{SXX}(\sum_{i=1}^n\frac{x_i^2}{n}-\bar{x}x_i+(-\bar{x}+x_i)x_j)
$$

We can create the desired $1/n$ term by adding and subtracting $n\bar{x}^2/n=\bar{x}^2$. We know that $\sum_{i=1}^nx_i^2+n\bar{x}^2=SXX$:

$$
h_{ij}=\frac{1}{SXX}(\frac{1}{n}(\sum x_i^2-n\bar{x}^2)+\bar{x}^2-\bar{x}x_i-\bar{x}x_j+x_ix_j)
$$

$$
h_{ij}=\frac{1}{n}+\frac{\bar{x}^2-\bar{x}x_i-\bar{x}x_j+x_ix_j}{SXX}
$$

We can factor the numerator into $(x_i-\bar{x})(x_j-\bar{x})$. We get our desired result:

$$
h_{ij}=\frac{1}{n}+\frac{(x_i-\bar{x})(x_j - \bar{x})}{SXX}
$$

d\)

#Work on#

6\)

a\)

The fifth point appears to be a bad leverage point, so a leverage point and an outlier. The first point is a appears to be a leverage point but a good one, it generally fits the pattern set by the next three points.

```{r echo=FALSE}
data <- data.frame(x = c(-3,-2,-1,0,6), y = c(10,6,5,3,12))

plot(x = data$x, y = data$y, xlab = "X Values", ylab = "Y Values", main = "Scatter Plot")
points(x = data$x[5], y = data$y[5], col = "red")
text(x = data$x[5], y = data$y[5], labels = "5", pos = 2)
```

b\)

```{r echo=FALSE}
data$y_hat <- 7.2 + .5*data$x
data$residuals <- data$y - data$y_hat
```

The predicted values $\hat{y}$ are $[5.7,6.2,6.7,7.2,10.2]^T$ so the residuals are:

$$
\hat{e}=y-\hat{y}=\begin{bmatrix} 10 \\ 6 \\ 5 \\ 3 \\12\end{bmatrix} -\begin{bmatrix} 5.7 \\ 6.2 \\ 6.7 \\ 7.2 \\ 10.2\end{bmatrix}=\begin{bmatrix} 4.3 \\ -0.2 \\ -1.7 \\ -4.2 \\ 1.8\end{bmatrix}
$$

c\)

$$
h_{ii}=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{SXX}
$$

```{r}
x_bar <- mean(data$x)
SXX <- sum((data$x-x_bar)^2)
n <- length(data$x)
data$leverage <- 1/n + (data$x - x_bar)^2/SXX
data$leverage
```

The leverage for each point is:

$$
h_{ii}=\begin{bmatrix}.38,.28,.22,.20,.92\end{bmatrix}
$$

The rule for a leverage point is $h_{ii}>\frac{4}{n}$. With $n=5$ the rule is $h_{ii}>.8$. So the only leverage point is point 5, $(6,10.2)$. This would be considered a bad leverage point as it doesn't fit the downward sloping pattern of the rest of the data.

d\)

The variance of each residual is $Var(\hat{e_i})=\sigma^2(1-h_{ii})$ as previously worked through in Question 4. We assume that $Y=X\beta +e$, so $Var(Y)=\sigma^2$. So $\sigma^2$ is the variance of $Y$, $\frac{\sum_{i=1}^n(y_i-\bar{y})^2}{n}$.

```{r}
y_bar <- mean(data$y)
var_y <- sum((data$y - y_bar)^2)/n
```

So $\sigma^2$ is `r var_y` .

```{r}
data$residual_variance <- var_y*(1-data$leverage)
```

The variances of the residuals are:

$$
Var(\hat{e})=\begin{bmatrix}6.7952,7.8912, 8.5488,8.7680,0.8768 \end{bmatrix}
$$

e\) #Work on

f\)

The highest leverage point has the smallest variance due to the relationship between $Var(\hat{e_i})$ and $h_{ii}$.

$$
Var(\hat{e_i})=\sigma^2(1-h_{ii})
$$

As $h_{ii}$ increases $Var(e_i)$ decreases.
