---
title: "STAT 608 HW 2"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 09/13/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

1\)

```{r}
x_i <- c(rep(1,5),rep(0,5))
sxx <- sum((x_i-mean(x_i))^2)
sxx


```

2\)

$$
Y=X\beta+e
$$

Where the design matrix X is:

$$
X=\begin{bmatrix} 1&1&0 \\ 1&0&1 \\ 1&1&1 \\1&2&2 \\ 1&2&1 \\1&1&2 \end{bmatrix}
$$

And vector $\beta$ is:

$$
\beta =\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix} 
$$The estimate for $\beta$ is:

$$
\hat{\beta}=(X^TX)^{-1}X^Ty
$$

3\)

a\)

The first parameter $\alpha_1$ should be the expected value of the first group $E[Y|x_1=1]$, so it should be the mean of the response variables with $x_1=1$. The second parameter $\alpha_2$ should be expected value of the second group $E[Y|x_2=1]$, so it should be the mean of the response variables with $x_2=1$.

b\)

In this case the design matrix X is the below:\

$$
X=\begin{bmatrix} x_{1,1} & x_{1,2} \\ x_{2,1} & x_{2,2} \\ \dots & \dots \\x_{n,1} & x_{n,2}\end{bmatrix}
$$

And we can say that there are m people in group 1 and n-m people in group 2. So:

$$
\sum_{i=1}^nx_{i,1}=m,\sum_{i=1}^n x_{i,2}=n-m
$$

We can use these facts to estimate $\hat{\alpha}$, the usual formula is below:

$$
\hat{\alpha}=(X^TX)^{-1}X^{T}y
$$

Where $X^{T}X$ is:

$$
X^TX=\begin{bmatrix} \sum_{i=1}^nx_{i,1}^2 &\sum_{i=1}^nx_{i,1}x_{i,2} \\ \sum_{i=1}^nx_{i,1}x_{i,2} & \sum_{i=1}^nx_{i,2}^2\end{bmatrix}=\begin{bmatrix} m & 0 \\ 0 & n-m\end{bmatrix}
$$

Then $(X^TX)^{-1}$ is:

$$
(X^TX)^{-1}=\frac{1}{m(n-m)}\begin{bmatrix} m & 0 \\ 0 & n-m\end{bmatrix}=\begin{bmatrix} \frac{1}{m} &0 \\ 0 &\frac{1}{n-m}\end{bmatrix}
$$

And $X^Ty$ is the below, where $\bar{y_1}$ and $\bar{y}_2$ are the mean responses for groups 1 and 2.

$$
X^Ty=\begin{bmatrix} \sum_{i=1}^nx_{i,1}y_i \\ \sum_{i=1}^nx_{i,2}y_i\end{bmatrix}=\begin{bmatrix} m \bar{y}_1 \\ (n-m)\bar{y}_2\end{bmatrix}
$$

So the final estimate, $\hat{\alpha}$ is:

$$
\hat{\alpha}=\begin{bmatrix} \bar{y}_1 \\ \bar{y}_2\end{bmatrix}
$$

This is consistent with what we stated in part a, the coefficients $\hat{\alpha_1},\hat{\alpha_2}$ is $E[Y|x_1=1]=\bar{y}_1$ and $E[Y|x_2=1]=\bar{y}_2$.

5\)

The correct answer is d.

RSS is the variability unexplained by the model, SSreg is the variability explained by the model. They total up to the SST, the total sample variability. So:

$$
\text{SST}=\text{SSreg}+\text{RSS}
$$

It is clear that model 1 has y values closer to the regression line than model 2, this indicates a smaller residual sum of squares in model 1. So:

$$
\text{RSS}_1<\text{RSS}_2
$$

Then to evaluate SSreg we can say:

$$
\text{SSreg}_1=\text{SST}-\text{RSS}_1
$$

and

$$
\text{SSreg}_2=\text{SST}-\text{RSS}_2
$$

Since $\text{RSS}_1<\text{RSS}_2$ we can conclude that SSreg for model 1 is greater than SSreg for model 2.

6\)

a\)

We want to show that $(y_i-\hat{y_i})=(y_i-\bar{y})-\hat{\beta_1}(x_i-\bar{x})$. Starting with $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$:

$$
(y_i-(\hat{\beta_0}+\hat{\beta_1}x_i))
$$

We know that $\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$ so:

$$
(y_i-(\bar{y}-\hat{\beta_1}\bar{x}+\hat{\beta_1}x_i))
$$

This leads us to our desired conclusion:

$$
(y_i-\bar{y})-\hat{\beta_1}(x_i-\bar{x})
$$

b\)

We want to show that $(\hat{y_i}-\bar{y})=\hat{\beta_1}(x_i-\bar{x})$. Starting with $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$:

$$
(\hat{\beta_0}+\hat{\beta_1}x_i-\bar{y})
$$

We know that $\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$ so:

$$
(\bar{y}-\hat{\beta_1}\bar{x}+\hat{\beta_1}x_i-\bar{y})
$$

This leads to:

$$
\hat{\beta_1}(x_i-\bar{x})
$$

c\)

Using our previous results we have:

$$
\sum_{i=1}^n(y_i-\hat{y_i})(\hat{y_i}-\bar{y})=\sum_{i=1}^n((y_i-\bar{y})-\hat{\beta_1}(x_i-\bar{x}))(\hat{\beta_1}(x_i-\bar{x}))
$$

$$
=\sum_{i=1}^n(\hat{\beta_1}(x_i-\bar{x})(y_i-\bar{y})-\hat{\beta_1}^2(x_i-\bar{x})^2)
$$

We know that $\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=SXY$ and $\sum_{i=1}^n(x_i-\bar{x})^2=SXX$. So we have:

$$
\hat{\beta_1}SXY-\hat{\beta_1}^2SXX
$$

Using the fact that $\hat{\beta_1}=\frac{SXY}{SXX}$:

$$
\sum_{i=1}^n(y_i-\hat{y_i})(\hat{y_i}-\bar{y})=\frac{SXY^2}{SXX}-\frac{SXY^2}{SXX}=0
$$ 8)

With x as a random n-dimensional vector and $E(x)=\mu$:

$$
\sum=E[(x-\mu)(x-\mu)^T]=E[x(x-\mu)^T-\mu(x-\mu)^T]
$$

$$
\sum=E[xx^T-x\mu^T-\mu x^T+\mu \mu^T]
$$

$$
\sum=E[xx^T]-E[x]\mu^T-\mu E[x^T]+\mu \mu^T
$$

Here we use that $E(x)=\mu$ then we get to the desired result:

$$
\sum=E[xx^T]-\mu\mu^T-\mu \mu^T +\mu \mu^T=E[x x^T]-\mu \mu^T
$$
