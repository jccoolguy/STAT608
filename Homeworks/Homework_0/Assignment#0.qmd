---
title: "STAT 639 HW 4"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 08/20/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

$$
\begin{bmatrix}
1&2&3\\
a&b&c
\end{bmatrix}
$$

Matrix Algebra Review

$A = \begin{bmatrix} 1&0&2&3\\-1&2&0&-2 \end{bmatrix}$, $B=\begin{bmatrix} 0&-1\\3&0\\2&1\\0&-2 \end{bmatrix}$ , $C=\begin{bmatrix} 1&0\\0&1\\0&0\end{bmatrix}$

1\.

$A'= \begin{bmatrix} 1 &-1\\0&2\\2&0\\3&-2\end{bmatrix}$

2\.

$A'+B=\begin{bmatrix} 1&-2\\3&2\\4&1\\3&-4\end{bmatrix}$

3\.

$AB=\begin{bmatrix} 4 &-5\\6&5 \end{bmatrix}$

4\.

$BA=\begin{bmatrix} 1&-2&0&2\\3&0&6&9\\1&2&4&4\\2&-4&0&4\end{bmatrix}$

$AB\neq BA$

5\.

$AB$ is not singular. The matrix is invertible due to the fact that the determinant is not zero.

$\text{det}(AB)=|AB|=(4)(5)-(-5)(6)=50$

6\.

The trace is the sum of diagonal elements. $\text{Tr}(AB) = 4 + 5=9$.

7\.

$(AB)'=B'A'$

8\.

$(AB)^{-1}=\frac{1}{50} \begin{bmatrix} 5&5 \\ -6 & 4 \end{bmatrix}$

9\.

$I_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$

10\.

$I_2 A=A$. The identity matrix multiplied by a matrix does not change the matrix.

11\.

The column space of C is a plane on XY.

12\.

The projection matrix for C is:

$\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$

13\.

$\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix}=\begin{bmatrix} 2 \\ 2 \\0 \end{bmatrix}$

14\.

The vector d pointing at $\begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix}$ has been projected down to the XY plane losing its Z axis direction. It now is a line with components $\begin{bmatrix} 2 \\ 2 \\0 \end{bmatrix}$.

15\.

No vector d and f are not orthogonal. The dot product $d'f=2$. In order for two vectors to be orthogonal the dot product must equal zero.

16\.

The dot product of $1 \cdot 1$ where $1 = \begin{bmatrix} 1 & 1 & \cdot & \cdot & 1\end{bmatrix}$ with length n is:

$1 \cdot 1=1_11_1+1_21_2+...1_n1_n=\sum_{i=1}^n1_i1_i=\sum_{i=1}^n1_i=n$

17\.

The dot product of $1 \cdot X=1_1X_1+1_2X_2+...+1_nX_n=\sum_{i=1}^nX_i$ .

18\.

The dot product of $X \cdot X=X_1X_1+X_2+...+X_nX_n=\sum_{i=1}^nX_i^2$.

19\.

The first eigenvector is multiplied by $\lambda$ when multiplied by A. We stay on the same eigenvector space changing direction/magnitude each time it is transformed by A.

II\. Calculus Review

1\.

$6x +2y^2$

2\.

$4xy -1$

III\. Log Review

1\.

$\text{log}(e)=1$

2\.

$\text{log}(\frac{x}{y})=\text{log}(x)-\text{log}(y)$

3\.

$\text{log}(x^n)=n\text{log}(x)$

4\.

$\text{log}(x)=y,x=e^y$

IV\. Statistics and Linear Regression Review

1\.

$\hat{y}=1.1667x-129.1667$

2\.

```{r}
1.1667*160 - 129.1667
```

3\.

As height increases by 1 cm the prediction of weight increases by 1.1667 kg.

4\.

The standard error of the slope is the measure of spread in the distribution of the height parameter estimate.

5\.

$t_{n-2}=\frac{\hat{\beta_1}-\beta_1}{se(\hat{\beta})}$

```{r}
t_statistic = (1.1667 - 0)/0.1521
t_statistic
```

6\.

Yes, height and weight are linearly associated. The height parameter estimate is statically significantly different than zero.

7\.

There is evidence as height increases weight tends to increase.

8\.

```{r}
t_alpha <- qt(1 - 0.05, 8 - 2)
lower <- 1.1667 - (0.1521/sqrt(8))*t_alpha
upper <- 1.1667 + (0.1521/sqrt(8))*t_alpha
c(lower, upper)
```

9\.

If we were to repeat this experiment again we are 95% confident that the estimate of the height parameter would rest between 0.86878 and 1.46462.
